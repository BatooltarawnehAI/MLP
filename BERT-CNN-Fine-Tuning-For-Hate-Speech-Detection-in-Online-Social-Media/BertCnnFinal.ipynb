{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media/blob/main/BertCnnFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KuTl1VnNfsX",
    "outputId": "8ecf776f-dbbc-4e5b-96f3-4158f3459a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.0\n",
      "  Using cached transformers-3.0.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from transformers==3.0.0) (1.26.4)\n",
      "Collecting tokenizers==0.8.0-rc4 (from transformers==3.0.0)\n",
      "  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from transformers==3.0.0) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers==3.0.0) (3.13.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers==3.0.0) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==3.0.0) (4.66.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers==3.0.0) (2023.10.3)\n",
      "Collecting sentencepiece (from transformers==3.0.0)\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting sacremoses (from transformers==3.0.0)\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==3.0.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==3.0.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers==3.0.0) (2024.8.30)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from sacremoses->transformers==3.0.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from sacremoses->transformers==3.0.0) (1.4.2)\n",
      "Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n",
      "Using cached emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[46 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /private/var/folders/vw/0c68s27d2h30zp4m2fpdx4kh0000gn/T/pip-build-env-_1vc0yea/overlay/lib/python3.12/site-packages/setuptools/dist.py:330: InformationOnly: Normalizing '0.8.0.rc4' to '0.8.0rc4'\n",
      "  \u001b[31m   \u001b[0m   self.metadata.version = self._normalize_version(self.metadata.version)\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/__init__.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.pyi -> build/lib.macosx-11.1-arm64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
      "\u001b[0mUsing device: mps\n",
      "Dataset size: 24783\n",
      "Epoch 1 of 3\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers==3.0.0 emoji\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the BERT-based model architecture\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), padding=(1, 0))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 1), stride=(1, 1))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc = nn.Linear(442, 3)  # Adjust if max_length changes\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        # Get all hidden states\n",
    "        outputs = self.bert(sent_id, attention_mask=mask, output_hidden_states=True)\n",
    "        all_layers = outputs.hidden_states\n",
    "\n",
    "        # Concatenate and process hidden layers\n",
    "        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in all_layers]), 0), 0, 1)\n",
    "        del all_layers\n",
    "        gc.collect()\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "        # CNN and fully connected layers\n",
    "        x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\n",
    "        x = self.flat(self.dropout(x))\n",
    "        x = self.fc(self.dropout(x))\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Preprocessing functions\n",
    "def read_dataset():\n",
    "    data = pd.read_csv(\"labeled_data.csv\")\n",
    "    data = data.drop(['count', 'hate_speech', 'offensive_language', 'neither'], axis=1)\n",
    "    print(f\"Dataset size: {len(data)}\")\n",
    "    return data['tweet'].tolist(), data['class']\n",
    "\n",
    "def pre_process_dataset(values):\n",
    "    processed_values = []\n",
    "    for value in values:\n",
    "        text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", value.lower())\n",
    "        text = re.sub(r\"([?.!,¿])\", r\" \", text)\n",
    "        text = \"\".join(l for l in text if l not in string.punctuation)\n",
    "        text = re.sub(r'[\" \"]+', \" \", text)\n",
    "        processed_values.append(text.strip())\n",
    "    return processed_values\n",
    "\n",
    "def data_process(data, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    for sentence in data:\n",
    "        bert_input = tokenizer(sentence, max_length=36, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        input_ids.append(bert_input['input_ids'].squeeze(0))\n",
    "        attention_masks.append(bert_input['attention_mask'].squeeze(0))\n",
    "\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    attention_masks = torch.stack(attention_masks)\n",
    "    labels = torch.tensor(labels.values if isinstance(labels, pd.Series) else labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "def load_and_process():\n",
    "    data, labels = read_dataset()\n",
    "    data = pre_process_dataset(data)\n",
    "    return data_process(data, labels)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_preds = 0, []\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        sent_id, mask, labels = [item.to(device) for item in batch]\n",
    "        model.zero_grad()\n",
    "\n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_preds.append(preds.detach().cpu())\n",
    "\n",
    "    return total_loss / len(train_dataloader), torch.cat(total_preds)\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, total_preds = 0, []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "            sent_id, mask, labels = [item.to(device) for item in batch]\n",
    "\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "            total_preds.append(preds.detach().cpu())\n",
    "\n",
    "    return total_loss / len(val_dataloader), torch.cat(total_preds)\n",
    "\n",
    "# Main script\n",
    "input_ids, attention_masks, labels = load_and_process()\n",
    "\n",
    "# Split the dataset\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.1, random_state=42)\n",
    "train_masks, val_masks = train_test_split(attention_masks, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = BERT_Arch(bert).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "    train_loss, _ = train()\n",
    "    val_loss, _ = evaluate()\n",
    "    print(f\"Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "# Evaluation on validation data\n",
    "val_loss, val_preds = evaluate()\n",
    "val_preds = torch.argmax(val_preds, axis=1)\n",
    "\n",
    "print(\"\\nValidation Performance:\")\n",
    "print(classification_report(val_labels, val_preds))\n",
    "print(f\"Accuracy: {accuracy_score(val_labels, val_preds):.4f}\")\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "print(\"Performance:\")\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print('Classification Report')\n",
    "print(classification_report(test_y, press))\n",
    "\n",
    "# print(\"Accuracy: \" + str(accuracy_score(test_y, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for MPS availability\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNAH18fNerSp+olcSBcN+Sm",
   "include_colab_link": true,
   "mount_file_id": "1vj3Z4UpPYqCamgeWQiN11TteiazAwTt5",
   "name": "BertCnnFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
